# -*- coding: utf-8 -*-
"""HW3 Problem4.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1xjsil88qNhWCX17o3NjX6fAtxLXQ7LIc
"""

import hw3_4_a_gendata
import numpy as np
import matplotlib.pyplot as plt
from sklearn.neural_network import MLPRegressor
from sklearn.model_selection import KFold


# generates data & split it into X (training input) and y (target output)
X, y = hw3_4_a_gendata.genDataSet(100)

neurons = 100  # <- number of neurons in the hidden layer
eta = 0.1       # <- the learning rate parameter

# here we create the MLP regressor
mlp =  MLPRegressor(hidden_layer_sizes=(neurons,), verbose=True, learning_rate_init=eta)
# here we train the MLP
mlp.fit(X, y)
# E_out in training
print("Training set score: %f" % mlp.score(X, y))

# now we generate new data as testing set and get E_out for testing set
X, y = hw3_4_a_gendata.genDataSet(100)
print("Testing set score: %f" % mlp.score(X, y))
ypred = mlp.predict(X)

plt.plot(X[:, 0], X[:, 1], '-')
plt.plot(X[:, 0], y, '-r')
plt.plot(X[:, 0], ypred, '-k')
plt.show()

import hw3_4_a_gendata
import numpy as np
import matplotlib.pyplot as plt
from sklearn.neural_network import MLPRegressor
from sklearn.model_selection import KFold

# number of samples
N = 10000

# generate data & split it into X (training input) and y (target output)
X, y = hw3_4_a_gendata.genDataSet(N)

# linear regression solution
w=np.linalg.pinv(X.T.dot(X)).dot(X.T).dot(y)


#neurons  <- number of neurons in the hidden layer
#eta  <- the learning rate parameter

bestNeurons=0
bestEta=0
bestScore=float('-inf')
score=0
for neurons in range(1,101,1):
  for eta in range(1,11,1):
    eta=eta/10.0
    kf = KFold(n_splits=10)
    cvscore=[]
    for train, validation in kf.split(X):
      X_train, X_validation, y_train, y_validation = X[train, :], X[validation, :], y[train], y[validation]
      # here we create the MLP regressor
      mlp =  MLPRegressor(hidden_layer_sizes=(neurons,), verbose=False, learning_rate_init=eta)
      # here we train the MLP
      mlp.fit(X_train, y_train)
      # now we get E_out for validation set
      score=mlp.score(X_validation, y_validation)
      cvscore.append(score)

    # average CV score
    score=sum(cvscore)/len(cvscore)
    if (score > bestScore):
      bestScore=score
      bestNeurons=neurons
      bestEta=eta
      print("Neurons " + str(neurons) + ", eta " + str(eta) + ". Testing set CV score: %f" % score)

#1000 sample output
#Neurons 63, eta 0.1. Testing set CV score: -2.555428
#10000 sample output
#Neurons 34, eta 0.1. Testing set CV score: -0.955031

# here we get a new training dataset
X, y = hw3_4_a_gendata.genDataSet(N)
# here we create the final MLP regressor
mlp =  MLPRegressor(hidden_layer_sizes=(bestNeurons,), verbose=True, learning_rate_init=bestEta)
# here we train the final MLP
mlp.fit(X, y)
# E_out in training
print("Training set score: %f" % mlp.score(X, y)) 
# here we get a new testing dataset
X, y = hw3_4_a_gendata.genDataSet(N)
# here test the final MLP regressor and get E_out for testing set
plt.figure(figsize=(10,10))
ypred=mlp.predict(X)
score=mlp.score(X, y)
print("Testing set score: %f" % score)
plt.plot(X[:, 0], X[:, 1], '.')
plt.plot(X[:, 0], y, 'rx')
plt.plot(X[:, 0], ypred, '-k')
ypredLR=X.dot(w)
plt.plot(X[:, 0], ypredLR, '--g')
plt.show()

